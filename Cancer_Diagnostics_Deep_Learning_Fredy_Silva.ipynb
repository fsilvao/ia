{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsilvao/ia/blob/main/Cancer_Diagnostics_Deep_Learning_Fredy_Silva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Power By... Fredy Silva O.**\n",
        "https://cl.linkedin.com/in/fredy-emprendedor\n"
      ],
      "metadata": {
        "id": "FrXxG7XDqXUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKCRxrYgncdO"
      },
      "outputs": [],
      "source": [
        "# Cargar Librerías\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cancer = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el conjunto de datos en entrenamiento y prueba con el 20% para prueba\n",
        "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(cancer.data, cancer.target, test_size=0.20, random_state=42)\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba con el 30% para prueba\n",
        "X_train_30, X_test_30, y_train_30, y_test_30 = train_test_split(cancer.data, cancer.target, test_size=0.30, random_state=42)\n",
        "\n",
        "# Mostrar el tamaño de los conjuntos de entrenamiento y prueba para ambas divisiones\n",
        "print(f\"Conjunto con el 20% para prueba: {X_train_20.shape[0]} en entrenamiento, {X_test_20.shape[0]} en prueba.\")\n",
        "print(f\"Conjunto con el 30% para prueba: {X_train_30.shape[0]} en entrenamiento, {X_test_30.shape[0]} en prueba.\")\n"
      ],
      "metadata": {
        "id": "4Z4_XDbSlDUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Crear el objeto StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador solo con el conjunto de datos de entrenamiento (puedes hacerlo para ambos casos)\n",
        "scaler.fit(X_train_20)\n",
        "\n",
        "# Aplicar la transformación tanto al conjunto de entrenamiento como al de prueba\n",
        "X_train_20_scaled = scaler.transform(X_train_20)\n",
        "X_test_20_scaled = scaler.transform(X_test_20)\n",
        "\n",
        "# Para el caso de 30% prueba, ajustar y transformar nuevamente si es necesario\n",
        "# Nota: No es necesario ajustar el escalador nuevamente si solo estás interesado en transformar los datos\n",
        "# con el mismo escalador ajustado previamente.\n",
        "X_train_30_scaled = scaler.transform(X_train_30)\n",
        "X_test_30_scaled = scaler.transform(X_test_30)"
      ],
      "metadata": {
        "id": "s_dHc5mxpoI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Crear el MLP con 3 capas ocultas, cada una con 30 neuronas\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(30, 30, 30), random_state=42)"
      ],
      "metadata": {
        "id": "kvlHza6urrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para jugar con la función de activación y el solver, puedes redefinir el clasificador así:\n",
        "mlp_tanh_adam = MLPClassifier(hidden_layer_sizes=(30, 30, 30), activation='tanh', solver='adam', random_state=42)\n",
        "mlp_relu_sgd = MLPClassifier(hidden_layer_sizes=(30, 30, 30), activation='relu', solver='sgd', random_state=42)\n",
        "mlp_logistic_lbfgs = MLPClassifier(hidden_layer_sizes=(30, 30, 30), activation='logistic', solver='lbfgs', random_state=42)"
      ],
      "metadata": {
        "id": "UHBU_t-Ys-jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Ejemplo para entrenar y evaluar el modelo tanh + adam\n",
        "mlp_tanh_adam.fit(X_train_20_scaled, y_train_20)\n",
        "y_pred = mlp_tanh_adam.predict(X_test_20_scaled)\n",
        "accuracy = accuracy_score(y_test_20, y_pred)\n",
        "\n",
        "print(f\"Accuracy of MLP with tanh activation and adam solver: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "tcR9YJ9WtEZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cargar el conjunto de datos y dividirlo\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.20, random_state=42)\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(30, 30, 30), activation='tanh', solver='adam', random_state=42, max_iter=1000)\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones sobre el conjunto de prueba\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "# Calcular y mostrar la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "0Y6UfFRDt68d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}